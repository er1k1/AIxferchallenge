# AIxferchallenge
STL10 Transfer Learning Project

This project uses the unlabeled and labeled images from the STL10 data set aquired from ImageNet. A description is given here https://cs.stanford.edu/~acoates/stl10/ . A Convolutional Neural Network is pre-trained using an autoencoder on the unlabeled images. Then the trained weights are fixed and further fully connected training layers are added which use the labeled training images starting with the pre-trained weights. 
For comparison the entire Convolutional Neural Network was trained solely on the same labeled images with random starting weights for 50 epochs.

The Python code for setting up and running the training excercise can be found in autocnn.py in the root directory. Data loading functions can be found in stl10_input.py in the dataset directory. Additional utility functions are located in utils.py in the utils directory. These local modules are imported by autocnn.py.
<a href="https://github.com/er1k1/AIxferchallenge/blob/master/model_final_arch.rtf">This document</a> shows the architecture of the model of which the first ten layers are used in pretraining. The weights are fixed before the final convolutional layer where the labeled training starts.
The trained models are serialised to the output/models directory and if they have already been created and exist in that directory, they are loaded directly instead of retraining. The model histories are also stored in the same place - giving loss curves over the training epochs for the three models (<a href="https://github.com/er1k1/AIxferchallenge/blob/master/autoencloss.png" >autoencoding model</a>, <a href= "https://github.com/er1k1/AIxferchallenge/blob/master/xferlearningloss.png">pretrained model</a> and <a href="https://github.com/er1k1/AIxferchallenge/blob/master/labelledtrainingloss.png">labelled training model</a>). The pre-training model has no validation data and the training loss decreases rapidly for the first 5 epochs and then steadily to 25 epochs. The transfer learning model shows a similar training loss curve to 30 epochs and a validation loss which follows the training loss but shows more variability over adjacent epochs. This may be expected but possibly shows a learning rate that is too high. Finally, the purely labelled training model shows a training loss which decreases linearly and a divergent validation loss which is unstable in shape. This shows overfitting as the learned model isn't representative of the test data and the validation loss actually increases. This is a good demonstration that the transfer learning model using an autoencoder to pre-train the model on unlabeled data has led to an improved model with lower variance.
A second run of the transfer learning model was performed in order to gain accuracy metrics shown in <a href="
https://github.com/er1k1/AIxferchallenge/blob/master/accuracy.png">this plot</a>. This shows a fairly low accuracy of 30 - 35% and rising at 25 epochs though it is lower than the training accuracy. Higher accuracy could be obtained by using the full unlabeled set for pre-training and by adding more convoultions - this requires extensive resources and training time.
